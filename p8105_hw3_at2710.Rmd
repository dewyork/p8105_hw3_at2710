---
title: "Homework 3"
author: "Anusorn Thanataveerat"
date: "October 3, 2018"
output: github_document
toc: true
toc_float: true
code_folding: hide
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(janitor)
library(knitr)
library(ggridges)
library(patchwork)
library(lubridate)

knitr::opts_chunk$set(echo = TRUE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
```

# Problem 1

```{r brfss}
data("brfss_smart2010")
health_overall <-
  brfss_smart2010 %>% clean_names() %>%  filter(str_detect(topic, c("Overall Health"))) %>%
  mutate(response = forcats::fct_relevel(response,
  c(
  "Excellent", "Very good", "Good", "Fair", "Poor"
  ))) %>% rename(state = locationabbr,
  county = locationdesc)
```

In 2002, which states were observed at 7 locations?

**Answer**: Florida, Connecticut and North Carolina.
```{r}
health_overall %>% filter(year == 2002) %>% distinct(state, county) %>% count(state) %>% 
  filter(n == 7) %>% kable()
```

Make a “spaghetti plot” that shows the number of observations in each state from 2002 to 2010.
```{r spaghetti_plot}
health_overall %>% group_by(year, state) %>% count(state) %>% ggplot(aes(x = year, y = n, group = state)) + geom_line(aes(color = state)) + theme(legend.position = "none") + ylab('Count') + xlab('Year')
```

**Answer**: There are a lot of observations from Florida in 2007 and 2010. 
```{r}
health_overall %>% group_by(year, state) %>% count(state) %>% ungroup() %>%  top_n(2)
```

Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.
```{r NY_plot}
health_overall %>% filter(year %in% c(2002, 2006, 2010) &
                            state == 'NY' &
                            response == 'Excellent') %>% group_by(year) %>% summarize(mean_excellent = mean(data_value),
                            sd_excellent = sd(data_value)) %>% kable(digits = 1)
```

**Answer**: The average proportion of respondents in New York claiming they are in excellent health is in a decline from year 2002 to 2010.

For each year and state, compute the average proportion in each response category (taking the average across locations in a state). 

```{r average_}
health_overall %>% group_by(year, state, response) %>% summarise(mean_proportion = mean(data_value, na.rm = TRUE)) %>% spread(key = response, value = mean_proportion) %>% kable(digits = 2)
```

Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r}
health_overall %>% group_by(year, state, response) %>% summarise(mean_proportion = mean(data_value, na.rm = TRUE)) %>% ggplot(aes(x = year, y = mean_proportion, color = state)) + geom_line() + 
  facet_wrap(. ~ response) + theme(legend.position = "none")
```

**Answer**: The majority of respondents give themselves a very good health and it has been consistent across the years and those rated themselves with poor health are the minority in every state and throughout the years 2002 to 2010.


# Problem 2

```{r pressure, echo=FALSE}
data("instacart")
```
**Answer**: The original dataset consists of `r nrow(instacart)` observations and `r ncol(instacart)` variables with details on the items customers buy on instacart website, including the time of the purchase and the duration from the last purchase at the store. The data is gathered from `r length(unique(instacart$user_id))` unique customers and the type of products with the highest purchase volume is `r names(sort(table(instacart$department), decreasing = TRUE)[1])` (`r round(sort(table(instacart$department), decreasing = TRUE)[1]*100/nrow(instacart),2)`% of the total volume). Among the  `r names(sort(table(instacart$department), decreasing = TRUE)[1])` category, here are the top 5 best selling items.

```{r top_five}
instacart %>% filter(department == 'produce') %>% count(product_name) %>% top_n(5) %>% kable()
```

The average number of items in each purchase is `r instacart %>% count(order_id) %>% summarise(mean = round(mean(n),1))`. We further look into what time of day customers usually place their order. And surprisingly, the orders mostly come in during 10am to 3pm, so people spend their working hours on buying groceries online!

```{r}
instacart %>% group_by(order_id) %>% count(order_hour_of_day) %>% ggplot(aes(order_hour_of_day)) + geom_histogram(binwidth = 0.4) + xlab('Hour of the day')
```

How many aisles are there, and which aisles are the most items ordered from?

**Answer**: There are `r length(table(instacart$aisle_id))` aisles and with aisle `r names(sort(table(instacart$aisle_id), decreasing = TRUE)[1])` having `r sort(table(instacart$aisle_id), decreasing = TRUE)[1]` items ordered from. ps. Aisle 83 is fresh vegetables so it makes sense that people would buy from this aise the most often given the short shelf life.

Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

```{r}
plot1 <-
  instacart %>% filter(aisle_id < 70) %>%  ggplot(aes(aisle_id)) + geom_histogram(binwidth = 0.9) + coord_flip() + xlab('Aisle ID') + scale_x_continuous(breaks =
  seq(0, 69, 5))
  plot2 <-
  instacart %>% filter(aisle_id >= 70) %>%  ggplot(aes(aisle_id)) + geom_histogram(binwidth = 0.9) + coord_flip() + xlab('') + scale_x_continuous(breaks =
  seq(70, 134, 5))
  plot1 + plot2
```

**Answer**: From the plot, we can see there are 4 aisles with the item counts exceed 50,000 units.(see table below). Again, these perishables have short shelf life and require more frequent re-orderings. 
```{r}
instacart %>% count(aisle) %>% filter(n > 50000) %>% kable()
```


Make a table showing the most popular item aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”

```{r}
instacart %>% filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% group_by(aisle) %>%  count(product_name) %>% top_n(1) %>% kable()
```


Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).


```{r}
instacart %>% filter(product_name %in% c('Pink Lady Apples', 'Coffee Ice Cream')) %>% group_by(product_name, order_dow) %>% summarise(mean_hour = round(mean(order_hour_of_day),1)) %>% spread(key = order_dow, value = mean_hour) %>% kable() 
```

**Answer** On most days of the week, people tend to purchase pink lady apples around noon while the coffee ice cream is usually ordered a little later, around early afternoon. ps. I was trying to find what each number of the variable 'order_dow' represents what day but couldn't find credible data dictionary (see link attached)
https://gist.github.com/jeremystan/c3b39d947d9b88b3ccff3147dbcf6c6b 


# Problem 3
This problem uses the NY NOAA data. DO NOT include this dataset in your local data directory; instead, load the data from the  p8105.datasets package (it’s called ny_noaa).
```{r problem_3, echo = FALSE}
data("ny_noaa")
cleaned_dat <-
  ny_noaa %>% filter(!(
  is.na(prcp) &
  is.na(snow) & is.na(snwd) & is.na(tmax) & is.na(tmin)
  )) %>%
  mutate(
  tmax = (as.numeric(tmax) / 10)*(9/5) + 32,
  tmin = (as.numeric(tmin) / 10)*(9/5) + 32,
  prcp = prcp / 254,
  snwd = snwd / 25.4,
  year = year(date),
  month = month(date),
  day = day(date),
  snow = snow / 25.4
  )
# separate(date, into = c("year", "month", "day"), sep="-") #output as chr
```

**Answer**: This is the weather data, with `r nrow(ny_noaa)` observations, which consists of daily information on precipition, snow and temperature from various locations. It's in a long format and hasn't been tidied since there are rows with no information on the weather. Thus we removed those observations with no data (`r ny_noaa %>% filter(is.na(prcp) & is.na(snow) & is.na(snwd) & is.na(tmax) & is.na(tmin)) %>% nrow()` rows) and ended up with the final dataset with `r nrow(cleaned_dat)` observations. The data comes from 747 unique weather stations in NY from year 1981 to 2010. There are a lot of missing data observed in the dataset particularly the tmax and tmin (`r round(100*sum(is.na(cleaned_dat$tmax))/nrow(cleaned_dat),0)`%). Another variable with dubious input is snow given there is one record, dated `r cleaned_dat %>% filter(snow < 0) %>% select(date)`, that shows the amount of snow in negative (`r cleaned_dat %>% filter(snow < 0) %>% select(snow)`). 

For snowfall, what are the most commonly observed values? Why?

**Answer**: It's `r names(sort(-table(cleaned_dat$snow)))[1]` since snow doesn't fall all year round, just during the winter season.

Make a two-panel plot showing the average temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r warning = FALSE}
cleaned_dat %>% group_by(month, year, id) %>% summarise(tmax_avg = mean(tmax, na.rm = TRUE),
  tmin_avg = mean(tmin, na.rm = TRUE)) %>% filter(month %in% c(1, 7)) %>% mutate(month_name = month.name[month]) %>% gather(key = max_min, value = temp, tmax_avg:tmin_avg) %>% ggplot(aes(x = year, y = temp, color = max_min)) + geom_jitter() +  facet_grid(. ~ month_name) + ylab('Temperature') + xlab('Year') + scale_color_manual(
  name = "",
  labels = c("Average Max",
  "Average Min"),
  values = c('tmax_avg' = 'red', 'tmin_avg' = 'blue')
  )
```

Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year. 

```{r warning = FALSE}
plot_a <- cleaned_dat %>% filter(!is.na(tmax) & !is.na(tmin)) %>% gather(key = max_min, value = temp, tmax:tmin) %>% ggplot(aes(x = date, y = temp, color = max_min)) + geom_smooth() + ylab('Temperature') + xlab('Date') + ggtitle('Daily Max and Min temperature (F)') + scale_color_manual(
  name = "",
  labels = c("Max temp.",
  "Min temp."),
  values = c('tmax' = 'red', 'tmin' = 'blue')
  )
plot_b <- cleaned_dat %>% filter(snow > 0 & snow < 100/25.4) %>% ggplot(aes(x = snow, fill = as.character(year))) +
  geom_density(alpha = .15, show.legend = FALSE) + 
  viridis::scale_fill_viridis(discrete = TRUE) + labs(x = 'Snow (in inches)', y = 'Density', title = 'Distribution of snowfall values greater than 0 and less than 4 inches by year') + theme_light()
#Create plot
plot_a + plot_b + plot_layout(ncol = 1, heights = c(1, 2))
```

