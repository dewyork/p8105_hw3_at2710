Homework 3
================
Anusorn Thanataveerat
October 3, 2018

true

Problem 1
=========

``` r
data("brfss_smart2010")
health_overall <-
  brfss_smart2010 %>% clean_names() %>%  filter(str_detect(topic, c("Overall Health"))) %>%
  mutate(response = forcats::fct_relevel(response,
  c(
  "Excellent", "Very good", "Good", "Fair", "Poor"
  ))) %>% rename(state = locationabbr,
  county = locationdesc)
```

In 2002, which states were observed at 7 locations?

**Answer**: Florida, Connecticut and North Carolina.

``` r
health_overall %>% filter(year == 2002) %>% distinct(state, county) %>% count(state) %>% 
  filter(n == 7) %>% kable()
```

| state |    n|
|:------|----:|
| CT    |    7|
| FL    |    7|
| NC    |    7|

Make a “spaghetti plot” that shows the number of observations in each state from 2002 to 2010.

``` r
health_overall %>% group_by(year, state) %>% count(state) %>% ggplot(aes(x = year, y = n, group = state)) + geom_line(aes(color = state)) + theme(legend.position = "none") + ylab('Count') + xlab('Year')
```

<img src="p8105_hw3_at2710_files/figure-markdown_github/spaghetti_plot-1.png" width="90%" />

**Answer**: There are a lot of observations from Florida in 2007 and 2010, 220 and 205 locations, respectively. I suspect whether it has anything to do with the non-election years.

``` r
health_overall %>% group_by(year, state) %>% count(state) %>% ungroup() %>%  top_n(2)
```

    ## Selecting by n

    ## # A tibble: 2 x 3
    ##    year state     n
    ##   <int> <chr> <int>
    ## 1  2007 FL      220
    ## 2  2010 FL      205

Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

``` r
health_overall %>% filter(year %in% c(2002, 2006, 2010) &
                            state == 'NY' &
                            response == 'Excellent') %>% group_by(year) %>% summarize(mean_excellent = mean(data_value),
                            sd_excellent = sd(data_value)) %>% kable(digits = 1)
```

|  year|  mean\_excellent|  sd\_excellent|
|-----:|----------------:|--------------:|
|  2002|             24.0|            4.5|
|  2006|             22.5|            4.0|
|  2010|             22.7|            3.6|

**Answer**: The average proportion of respondents in New York claiming they are in excellent health is in a decline from year 2002 to 2010.

For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

``` r
health_overall %>% group_by(year, state, response) %>% summarise(mean_proportion = mean(data_value, na.rm = TRUE)) %>% ggplot(aes(x = year, y = mean_proportion, color = state)) + geom_line() + 
  facet_wrap(. ~ response) + theme(legend.position = "none")
```

<img src="p8105_hw3_at2710_files/figure-markdown_github/average_-1.png" width="90%" />

**Answer**: The majority of respondents give themselves a very good health and it has been consistent across the years and those rated themselves with poor health are the minority in every state and throughout the years 2002 to 2010.

Problem 2
=========

**Answer**: The original dataset consists of 1384617 observations and 15 variables with details on the items customers buy on instacart website, including the time of the purchase and the duration from the last purchase at the store. The data is gathered from 131209 unique customers and the type of products with the highest purchase volume is produce (29.55% of the total volume). Among the produce category, here are the top 5 best selling items.

``` r
instacart %>% filter(department == 'produce') %>% count(product_name) %>% top_n(5) %>% kable()
```

    ## Selecting by n

| product\_name          |      n|
|:-----------------------|------:|
| Bag of Organic Bananas |  15480|
| Banana                 |  18726|
| Large Lemon            |   8135|
| Organic Baby Spinach   |   9784|
| Organic Strawberries   |  10894|

The average number of items in each purchase is 10.6. We further look into what time of day customers usually place their order. And surprisingly, the orders mostly come in during 10am to 3pm, so people spend their working hours on buying groceries online!

``` r
instacart %>% group_by(order_id) %>% count(order_hour_of_day) %>% ggplot(aes(order_hour_of_day)) + geom_histogram(binwidth = 0.4) + xlab('Hour of the day')
```

<img src="p8105_hw3_at2710_files/figure-markdown_github/unnamed-chunk-3-1.png" width="90%" />

How many aisles are there, and which aisles are the most items ordered from?

**Answer**: There are 134 aisles and with aisle 83 having 150609 items ordered from. ps. Aisle 83 is fresh vegetables so it makes sense that people would buy from this aise the most often given the short shelf life.

Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

``` r
plot1 <-
  instacart %>% filter(aisle_id < 70) %>%  ggplot(aes(aisle_id)) + geom_histogram(binwidth = 0.9) + coord_flip() + xlab('Aisle ID') + ylab('Numbers of times ordered') + scale_x_continuous(breaks =
  seq(0, 69, 5))
  plot2 <-
  instacart %>% filter(aisle_id >= 70) %>%  ggplot(aes(aisle_id)) + geom_histogram(binwidth = 0.9) + coord_flip() + xlab('') + ylab('Numbers of times ordered') + scale_x_continuous(breaks =
  seq(70, 134, 5))
  plot1 + plot2
```

<img src="p8105_hw3_at2710_files/figure-markdown_github/unnamed-chunk-4-1.png" width="90%" />

**Answer**: From the plot, we can see there are 4 aisles with the item counts exceed 50,000 units.(see table below). Again, these perishables have short shelf life and require more frequent re-order.

``` r
instacart %>% count(aisle) %>% filter(n > 50000) %>% kable()
```

| aisle                      |       n|
|:---------------------------|-------:|
| fresh fruits               |  150473|
| fresh vegetables           |  150609|
| packaged vegetables fruits |   78493|
| yogurt                     |   55240|

Make a table showing the most popular item aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”

``` r
instacart %>% filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% group_by(aisle) %>%  count(product_name) %>% top_n(1) %>% kable()
```

    ## Selecting by n

| aisle                      | product\_name                                 |     n|
|:---------------------------|:----------------------------------------------|-----:|
| baking ingredients         | Light Brown Sugar                             |   499|
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |    30|
| packaged vegetables fruits | Organic Baby Spinach                          |  9784|

**Answer**:Since I don't bake nor do I own dog I don't know if it makes sense for the light brown sugar and the chicken and rice recipe dog treats to be the best seller. But I agree that baby spinach deserves its place in the packaged veggies/fruits section.

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

``` r
instacart %>% filter(product_name %in% c('Pink Lady Apples', 'Coffee Ice Cream')) %>% group_by(product_name, order_dow) %>% summarise(mean_hour = round(mean(order_hour_of_day),1)) %>% spread(key = order_dow, value = mean_hour) %>% kable() 
```

| product\_name    |     0|     1|     2|     3|     4|     5|     6|
|:-----------------|-----:|-----:|-----:|-----:|-----:|-----:|-----:|
| Coffee Ice Cream |  13.8|  14.3|  15.4|  15.3|  15.2|  12.3|  13.8|
| Pink Lady Apples |  13.4|  11.4|  11.7|  14.2|  11.6|  12.8|  11.9|

**Answer** On most days of the week, people tend to purchase pink lady apples around noon while the coffee ice cream is usually ordered a little later, around early afternoon. ps. I was trying to find what each number of the variable 'order\_dow' represents what day but couldn't find credible data dictionary (see link attached) <https://gist.github.com/jeremystan/c3b39d947d9b88b3ccff3147dbcf6c6b>

Problem 3
=========

This problem uses the NY NOAA data. DO NOT include this dataset in your local data directory; instead, load the data from the p8105.datasets package (it’s called ny\_noaa).

**Answer**: This is the weather data, with 2595176 observations, which consists of daily information on precipition, snow and temperature from various locations. It's in a long format and hasn't been tidied since there are rows with no information on the weather. Thus we removed those observations with no data (72278 rows) and ended up with the final dataset with 2522898 observations. The data comes from 747 unique weather stations in NY from year 1981 to 2010. There are a lot of missing data observed in the dataset particularly the tmax and tmin (42%). Another variable with dubious input is snow given there is one record, dated 2005-06-15, that shows the amount of snow in negative (-0.5).

For snowfall, what are the most commonly observed values? Why?

**Answer**: It's 0 since snow doesn't fall all year round, just during the winter season.

Make a two-panel plot showing the average temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

``` r
cleaned_dat %>% group_by(month, year, id) %>% summarise(tmax_avg = mean(tmax, na.rm = TRUE),
  tmin_avg = mean(tmin, na.rm = TRUE)) %>% filter(month %in% c(1, 7)) %>% mutate(month_name = month.name[month]) %>% gather(key = max_min, value = temp, tmax_avg:tmin_avg) %>% ggplot(aes(x = year, y = temp, color = max_min)) + geom_jitter() + geom_smooth() +  facet_grid(. ~ month_name) + ylab('Temperature') + xlab('Year') + scale_color_manual(
  name = "",
  labels = c("Average Max",
  "Average Min"),
  values = c('tmax_avg' = 'firebrick2', 'tmin_avg' = 'steelblue2')
  )
```

    ## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'

<img src="p8105_hw3_at2710_files/figure-markdown_github/unnamed-chunk-8-1.png" width="90%" />

**Answer**:Clearly the average min and max temperature in July are higher than those in January. Also, there seems to be more variability, across stations, in average min/max temperature in January compared to July; there are a lot of overlap between some locations' average max and min temperature in January while there's a clear break of min/max temperature across locations in July. In January, there are some locations with extremely low average min temperature. In the year 2005, one location was recorded with average minimum temperature of -18.04 degrees, the lowest average ever recorded in this dataset. We also noticed that the average temperatures are lower than usual in January 1994.

Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

``` r
plot_a <- cleaned_dat %>% filter(!is.na(tmax) & !is.na(tmin)) %>% gather(key = max_min, value = temp, tmax:tmin) %>% ggplot(aes(x = date, y = temp, color = max_min)) + geom_smooth() + ylab('Temperature') + xlab('Date') + ggtitle('Daily Max and Min temperature (F)') + scale_color_manual(
  name = "",
  labels = c("Max temp.",
  "Min temp."),
  values = c('tmax' = 'red', 'tmin' = 'blue')
  )
plot_b <- cleaned_dat %>% filter(snow > 0 & snow < 100/25.4) %>% ggplot(aes(x = snow, fill = as.character(year))) +
  geom_density(alpha = .15, show.legend = FALSE) + 
  viridis::scale_fill_viridis(discrete = TRUE) + labs(x = 'Snow (in inches)', y = 'Density', title = 'Distribution of snowfall less than 4 inches by year') + theme_light()
#Create plot
plot_a + plot_b + plot_layout(ncol = 1, heights = c(1, 2))
```

    ## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'

<img src="p8105_hw3_at2710_files/figure-markdown_github/unnamed-chunk-9-1.png" width="90%" />

**Answer**: The key takeaways from these figures are the temperature measured in New York has been climbing from 1990 to 2010. For the snowfall plot, It shows that each year, most of the time when it snows (between 0 and 4 inches), it usually does less than an inch. Also we noticed the peak around 1, 2 and 3 inches; this is probably due to the rounding up of the numbers reported.
